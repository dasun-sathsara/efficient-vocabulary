{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenizing`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def get_words_nltk(words: list[str]) -> list[str]:\n",
    "    final_words = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    full_string = \"\\n\".join(words)\n",
    "    words = word_tokenize(full_string)\n",
    "    filtered_words = set(map(lambda x: x.lower(), words)).difference(set(stopwords.words(\"english\")))\n",
    "    tagged_words = nltk.pos_tag(filtered_words)\n",
    "\n",
    "    for word_tag in tagged_words:\n",
    "        word, tag = word_tag\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            # noun\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"n\")\n",
    "        elif tag.startswith(\"JJ\"):\n",
    "            # adjective\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"a\")\n",
    "        elif tag.startswith(\"VB\"):\n",
    "            # verb\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"v\")\n",
    "        elif tag.startswith(\"RB\"):\n",
    "            # adverb\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"r\")\n",
    "        else:\n",
    "            lemmatized_word = word\n",
    "\n",
    "        # print(f'{word} -> {lemmatized_word}')\n",
    "        final_words.append(lemmatized_word)\n",
    "\n",
    "\n",
    "get_words_nltk([\"You want to play games?\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "s = word_tokenize(\n",
    "    \"Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string:.\"\n",
    ")\n",
    "filtered_words = set(map(lambda x: x.lower(), s)).difference(stop_words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"tokenizers\", \"n\"))\n",
    "\n",
    "# print(len(s))\n",
    "# print(len(filtered_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efficient-vocabulary-7xDIPokp-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f646a4e882ec5509152bc772aedab0f11df437a23f03603f53b4c8c0dfe5bc7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
