{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenizing`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def get_words_nltk(words: list[str]) -> list[str]:\n",
    "    final_words = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    full_string = \"\\n\".join(words)\n",
    "    words = word_tokenize(full_string)\n",
    "    filtered_words = set(map(lambda x: x.lower().replace(\".\", \"\").replace(\"?\", \"\"), words)).difference(set(stopwords.words(\"english\")))\n",
    "    tagged_words = nltk.pos_tag(filtered_words)\n",
    "\n",
    "    for word_tag in tagged_words:\n",
    "        word, tag = word_tag\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            # noun\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"n\")\n",
    "        elif tag.startswith(\"JJ\"):\n",
    "            # adjective\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"a\")\n",
    "        elif tag.startswith(\"VB\"):\n",
    "            # verb\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"v\")\n",
    "        elif tag.startswith(\"RB\"):\n",
    "            # adverb\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"r\")\n",
    "        else:\n",
    "            lemmatized_word = word\n",
    "            # print(lemmatized_word,tag)\n",
    "\n",
    "        print(f\"{word} ({tag}) -> {lemmatized_word}\")\n",
    "        # print(f\"{lemmatized_word}\")\n",
    "        final_words.append(lemmatized_word)\n",
    "\n",
    "    return list(final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def parse_subtitle(fp: Path) -> list[str]:\n",
    "    \"\"\"Parse the .srt to file and extract text\n",
    "\n",
    "    Args:\n",
    "        fp (Path): Path object pointing to the subtitle file\n",
    "    \"\"\"\n",
    "    with open(fp, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        line: str = unicodedata.normalize(\"NFKC\", line.strip())\n",
    "        if not (line.isdigit() or \"-->\" in line):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    return filtered_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = r'C:\\Users\\evonD\\Documents\\WORK\\Projects\\Python\\Projects\\Efficient Vocabulary'\n",
    "parent_path = Path(parent_path)\n",
    "all_words_path = parent_path / \"word_data\" / \"all_words.txt\"\n",
    "common_words_path = parent_path / \"word_data\" / \"common_words.txt\"\n",
    "\n",
    "all_words = set(all_words_path.read_text().split(\"\\n\"))\n",
    "common_words = set(common_words_path.read_text().split(\"\\n\"))\n",
    "\n",
    "# return set(words).intersection(all_words).difference(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = get_words_nltk(parse_subtitle(\"../subtitles/sample_subtitle.srt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = set(words).intersection(all_words).difference(common_words)\n",
    "\n",
    "for i in f:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"?key.\".replace(\".\", \"\").replace(\"?\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "s = word_tokenize(\n",
    "    \"Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string:.\"\n",
    ")\n",
    "filtered_words = set(map(lambda x: x.lower(), s)).difference(stop_words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"tokenizers\", \"n\"))\n",
    "\n",
    "# print(len(s))\n",
    "# print(len(filtered_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efficient-vocabulary-7xDIPokp-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f646a4e882ec5509152bc772aedab0f11df437a23f03603f53b4c8c0dfe5bc7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
