{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenizing`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def get_words_nltk(words: list[str]) -> list[str]:\n",
    "    final_words = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    full_string = \"\\n\".join(words)\n",
    "    words = word_tokenize(full_string)\n",
    "    filtered_words = set(map(lambda x: x.lower().replace(\".\", \"\").replace(\"?\", \"\"), words)).difference(set(stopwords.words(\"english\")))\n",
    "    tagged_words = nltk.pos_tag(filtered_words)\n",
    "\n",
    "    for word_tag in tagged_words:\n",
    "        word, tag = word_tag\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            # noun\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"n\")\n",
    "        elif tag.startswith(\"JJ\"):\n",
    "            # adjective\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"a\")\n",
    "        elif tag.startswith(\"VB\"):\n",
    "            # verb\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"v\")\n",
    "        elif tag.startswith(\"RB\"):\n",
    "            # adverb\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, \"r\")\n",
    "        else:\n",
    "            lemmatized_word = word\n",
    "            # print(lemmatized_word,tag)\n",
    "\n",
    "        print(f\"{word} ({tag}) -> {lemmatized_word}\")\n",
    "        # print(f\"{lemmatized_word}\")\n",
    "        final_words.append(lemmatized_word)\n",
    "\n",
    "    return list(final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def parse_subtitle(fp: Path) -> list[str]:\n",
    "    \"\"\"Parse the .srt to file and extract text\n",
    "\n",
    "    Args:\n",
    "        fp (Path): Path object pointing to the subtitle file\n",
    "    \"\"\"\n",
    "    with open(fp, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        line: str = unicodedata.normalize(\"NFKC\", line.strip())\n",
    "        if not (line.isdigit() or \"-->\" in line):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    return filtered_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = r'C:\\Users\\evonD\\Documents\\WORK\\Projects\\Python\\Projects\\Efficient Vocabulary'\n",
    "parent_path = Path(parent_path)\n",
    "all_words_path = parent_path / \"word_data\" / \"all_words.txt\"\n",
    "common_words_path = parent_path / \"word_data\" / \"common_words.txt\"\n",
    "\n",
    "all_words = set(all_words_path.read_text().split(\"\\n\"))\n",
    "common_words = set(common_words_path.read_text().split(\"\\n\"))\n",
    "\n",
    "# return set(words).intersection(all_words).difference(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = get_words_nltk(parse_subtitle(\"../subtitles/sample_subtitle.srt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stale\n",
      "genuinely\n",
      "nasty\n",
      "jersey\n",
      "warrior\n",
      "tentacle\n",
      "darker\n",
      "de\n",
      "badass\n",
      "ai\n",
      "impressed\n",
      "id\n",
      "chaos\n",
      "max\n",
      "sum\n",
      "listener\n",
      "supplement\n",
      "bye\n",
      "miserable\n",
      "execute\n",
      "brainstorm\n",
      "fugitive\n",
      "conclusive\n",
      "propaganda\n",
      "america\n",
      "darn\n",
      "sees\n",
      "mascot\n",
      "fry\n",
      "someplace\n",
      "ca\n",
      "cosmic\n",
      "breathing\n",
      "vat\n",
      "glider\n",
      "eel\n",
      "ms\n",
      "jameson\n",
      "condominium\n",
      "cult\n",
      "ta\n",
      "felt\n",
      "god\n",
      "shield\n",
      "hustle\n",
      "rap\n",
      "elemental\n",
      "stephen\n",
      "stimulate\n",
      "reckless\n",
      "em\n",
      "font\n",
      "queen\n",
      "resurrect\n",
      "dangle\n",
      "stark\n",
      "elf\n",
      "ock\n",
      "octavius\n",
      "aah\n",
      "counsel\n",
      "accessed\n",
      "nana\n",
      "tony\n",
      "sabbatical\n",
      "phew\n",
      "handsome\n",
      "idiot\n",
      "menace\n",
      "brag\n",
      "weird\n",
      "relentless\n",
      "reconsider\n",
      "gross\n",
      "fury\n",
      "accidentally\n",
      "catastrophic\n",
      "buddy\n",
      "cooler\n",
      "honestly\n",
      "tin\n",
      "legally\n",
      "ugh\n",
      "statue\n",
      "superhero\n",
      "brace\n",
      "wo\n",
      "shh\n",
      "london\n",
      "footage\n",
      "casually\n",
      "monster\n",
      "pry\n",
      "frozen\n",
      "ben\n",
      "comm\n",
      "dodged\n",
      "deejay\n",
      "zap\n",
      "spider\n",
      "offend\n",
      "villain\n",
      "dumber\n",
      "smiling\n",
      "pas\n",
      "condo\n",
      "mommy\n",
      "cosmos\n",
      "friends\n",
      "sweetest\n",
      "sugarcoat\n",
      "rack\n",
      "bitter\n",
      "lapdog\n",
      "webs\n",
      "tamper\n",
      "mit\n",
      "vigilante\n",
      "loyalty\n",
      "madness\n",
      "legs\n",
      "goddamn\n",
      "huh\n",
      "grateful\n",
      "nonsense\n",
      "daisy\n",
      "flung\n",
      "bleed\n",
      "dissipate\n",
      "dessert\n",
      "avenger\n",
      "relieve\n",
      "displacement\n",
      "sanctum\n",
      "fling\n",
      "guardian\n",
      "junk\n",
      "devastate\n",
      "fabricator\n",
      "congratulation\n",
      "norman\n",
      "cyborg\n",
      "bugle\n",
      "goblin\n",
      "deed\n",
      "otherworldly\n",
      "infinite\n",
      "cease\n",
      "hurting\n",
      "endanger\n",
      "rageful\n",
      "undercroft\n",
      "dude\n",
      "shooter\n",
      "nick\n",
      "goo\n",
      "spiral\n",
      "cultists\n",
      "trespasser\n",
      "po\n",
      "dragged\n",
      "respectfully\n",
      "disturb\n",
      "humiliation\n",
      "disturbance\n",
      "murderer\n",
      "fascinate\n",
      "mets\n",
      "nephew\n",
      "jane\n",
      "sounds\n",
      "di\n",
      "peddle\n",
      "fought\n",
      "taught\n",
      "curt\n",
      "purple\n",
      "dinosaur\n",
      "lola\n",
      "goodbyes\n",
      "zero\n",
      "ritual\n",
      "told\n",
      "revelation\n",
      "oasis\n",
      "ton\n",
      "ned\n",
      "donkey\n",
      "reactor\n",
      "magic\n",
      "coordinate\n",
      "lame\n",
      "canyon\n",
      "hassle\n",
      "unreasonable\n",
      "conspiracy\n",
      "calm\n",
      "excuse\n",
      "russian\n",
      "yep\n",
      "emcee\n",
      "scrap\n",
      "j\n",
      "injured\n",
      "mistrust\n",
      "safer\n",
      "insane\n",
      "na\n",
      "saw\n",
      "tis\n",
      "wha\n",
      "mary\n",
      "praise\n",
      "fixed\n",
      "misguide\n",
      "greedy\n",
      "countless\n",
      "liberty\n",
      "fluid\n",
      "wong\n",
      "outdone\n",
      "dine\n",
      "harold\n",
      "fool\n",
      "thirsty\n",
      "explaining\n",
      "ruin\n",
      "hulk\n",
      "spell\n",
      "alien\n",
      "billionaire\n",
      "backup\n",
      "reverse\n",
      "infinitely\n",
      "annoyed\n",
      "grown\n",
      "rip\n",
      "doc\n",
      "sidetrack\n",
      "f\n",
      "reporting\n",
      "decoration\n",
      "blizzard\n",
      "crawl\n",
      "dragon\n",
      "gwen\n",
      "commence\n",
      "casualty\n",
      "apologize\n",
      "amendment\n",
      "portal\n",
      "becomes\n",
      "gon\n",
      "smash\n",
      "doughnut\n",
      "flint\n",
      "lumber\n",
      "dive\n",
      "monthly\n",
      "cobweb\n",
      "uh\n",
      "calamity\n",
      "pilate\n",
      "men\n",
      "pathetic\n",
      "octopus\n",
      "drowned\n",
      "empire\n",
      "struck\n",
      "brutal\n",
      "fell\n",
      "gateways\n",
      "repair\n",
      "dumb\n",
      "tingle\n",
      "jones\n",
      "accomplice\n",
      "ow\n",
      "blast\n",
      "york\n",
      "caused\n",
      "drawer\n",
      "kong\n",
      "theorist\n",
      "watson\n",
      "mmmm\n",
      "heh\n",
      "organisms\n",
      "fusion\n",
      "earlier\n",
      "flip\n",
      "calculus\n",
      "sacrifice\n",
      "unavailable\n",
      "brainwash\n",
      "subtract\n",
      "dr\n",
      "tan\n",
      "ambulance\n",
      "hogan\n",
      "curse\n",
      "punch\n",
      "wrist\n",
      "jr\n",
      "compromise\n",
      "wow\n",
      "cure\n",
      "eyes\n",
      "hypnotize\n",
      "heartbreaking\n",
      "lizard\n",
      "teeth\n",
      "overload\n",
      "suck\n",
      "disappointed\n",
      "cowardly\n",
      "rattle\n",
      "alarm\n",
      "stuck\n",
      "equalizer\n",
      "choke\n",
      "bachelor\n",
      "supreme\n",
      "gal\n",
      "um\n",
      "nicky\n",
      "tiger\n",
      "matt\n",
      "protector\n",
      "frighteningly\n",
      "insincerely\n",
      "lethal\n",
      "nickname\n",
      "plug\n",
      "complicate\n",
      "eddie\n",
      "facial\n",
      "conquer\n",
      "perish\n",
      "robot\n",
      "arc\n",
      "deactivate\n",
      "seduce\n",
      "delinquent\n",
      "fancy\n",
      "hallway\n",
      "frisbee\n",
      "clown\n",
      "deadly\n",
      "wicked\n",
      "fiasco\n",
      "renovation\n",
      "disappointment\n",
      "siberia\n",
      "sickness\n",
      "whew\n",
      "plead\n",
      "boston\n",
      "shovel\n",
      "chrysler\n",
      "otto\n",
      "swarm\n",
      "terrorize\n",
      "crypt\n",
      "amaze\n",
      "michelle\n",
      "rewind\n",
      "criticizing\n",
      "tease\n",
      "pragmatic\n",
      "hm\n",
      "burrito\n",
      "punk\n",
      "flex\n",
      "web\n",
      "brewster\n",
      "parker\n",
      "seizure\n",
      "entrust\n",
      "tow\n",
      "beck\n",
      "aka\n",
      "mr\n",
      "attaboy\n",
      "embarrass\n",
      "pastor\n",
      "hack\n",
      "sorcerer\n",
      "snack\n",
      "neat\n",
      "decent\n",
      "longer\n",
      "weakness\n",
      "vigilantism\n",
      "rehash\n",
      "wanted\n",
      "desecration\n",
      "grid\n",
      "yo\n",
      "internet\n",
      "wandered\n",
      "thompson\n",
      "forgive\n",
      "jonah\n",
      "posse\n",
      "whoa\n",
      "lure\n",
      "fred\n",
      "dungeon\n",
      "protestors\n",
      "conceals\n",
      "da\n",
      "craziest\n",
      "till\n",
      "squeak\n",
      "sewer\n",
      "ensue\n",
      "landmark\n",
      "puke\n",
      "rent\n",
      "fed\n",
      "sideways\n",
      "sooner\n",
      "yous\n",
      "contend\n",
      "stiff\n",
      "peter\n",
      "rhinoceros\n",
      "wan\n",
      "ridiculous\n",
      "adjective\n",
      "unpunished\n",
      "scour\n",
      "multiverse\n",
      "blipped\n",
      "tune\n",
      "dialect\n",
      "cape\n",
      "advisement\n",
      "warrant\n",
      "securing\n",
      "incriminate\n",
      "liability\n",
      "technicality\n",
      "sec\n",
      "corpse\n",
      "cartridge\n",
      "screw\n",
      "flaunt\n",
      "optimism\n",
      "awkward\n",
      "existential\n",
      "ribs\n",
      "impale\n",
      "admits\n",
      "seal\n",
      "geometry\n",
      "yorkers\n",
      "flash\n",
      "rehearse\n",
      "minus\n",
      "jolly\n",
      "botch\n",
      "underestimated\n",
      "commute\n",
      "wizard\n",
      "random\n",
      "knew\n",
      "unmask\n",
      "halloween\n",
      "messy\n",
      "damn\n",
      "evenly\n",
      "mixer\n",
      "torture\n",
      "leeds\n",
      "monkey\n",
      "ass\n",
      "morality\n",
      "parameter\n",
      "pi\n",
      "marko\n",
      "drone\n",
      "coward\n",
      "ouch\n",
      "magician\n",
      "mm\n",
      "skip\n",
      "ho\n",
      "vulture\n",
      "trap\n",
      "excite\n",
      "doorbell\n",
      "deserves\n",
      "rampage\n",
      "stab\n",
      "preach\n",
      "vile\n",
      "sandman\n",
      "blockbuster\n",
      "midtown\n",
      "slimy\n",
      "aunt\n",
      "chancellor\n",
      "tingling\n",
      "intersection\n",
      "rhyme\n",
      "archimedean\n",
      "relic\n",
      "mister\n",
      "uncover\n",
      "radius\n",
      "subway\n",
      "advertise\n",
      "isolate\n",
      "mes\n",
      "rotunda\n",
      "scroll\n",
      "curve\n",
      "stricken\n",
      "confuse\n",
      "unbelievable\n",
      "vice\n",
      "diagnostic\n",
      "pernicious\n",
      "frankly\n",
      "corrupt\n",
      "microwave\n",
      "connects\n",
      "anonymous\n",
      "happens\n",
      "rune\n",
      "coolest\n",
      "stole\n",
      "hunt\n",
      "mighty\n",
      "la\n",
      "mud\n"
     ]
    }
   ],
   "source": [
    "f = set(words).intersection(all_words).difference(common_words)\n",
    "\n",
    "for i in f:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"?key.\".replace(\".\", \"\").replace(\"?\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "s = word_tokenize(\n",
    "    \"Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string:.\"\n",
    ")\n",
    "filtered_words = set(map(lambda x: x.lower(), s)).difference(stop_words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"tokenizers\", \"n\"))\n",
    "\n",
    "# print(len(s))\n",
    "# print(len(filtered_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efficient-vocabulary-7xDIPokp-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f646a4e882ec5509152bc772aedab0f11df437a23f03603f53b4c8c0dfe5bc7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
